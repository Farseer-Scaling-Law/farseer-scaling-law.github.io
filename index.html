<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Predictable Scale: Part I — Optimal Hyperparameter Scaling Law in
        Large Language Model Pretraining">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Predictable Scale: Part I</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$', right: '$', display: false}
      ]
    });
  });
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <span class="highlight-title">Predictable Scale: Part I — Optimal Hyperparameter Scaling Law in 
            Large Language Model Pretraining</span>
          </h1>
          
            <style>
            /* 动态渐变效果 */
            .highlight-title {
              background: linear-gradient(
                45deg,
                #003f5c 0%,
                #374c80 12.5%,
                #7a5195 25%,
                #bc5090 37.5%,
                #ef5675 50%,
                #f97170 62.5%,
                #ff7c43 75%,
                #ffa600 87.5%
              );
              background-size: 400% 400%;
              -webkit-background-clip: text;
              background-clip: text;
              color: transparent;
              font-style: italic;
              animation: gradientShift 12s ease infinite;
              text-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }

            @keyframes gradientShift {
              0% { background-position: 0% 50%; }
              50% { background-position: 100% 50%; }
              100% { background-position: 0% 50%; }
            }
            
            @media (prefers-reduced-motion: reduce) {
              .highlight-title {
                animation: none;
                background: #FF6B6B; /* 降级为纯色 */
              }
            }
            /* 保持原有字体样式 */
            .publication-title {
              font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            }
            </style>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span class="author-name">Houyi Li</span><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <span class="author-name">Wenzhen Zheng</span><sup>1</sup>,
            </span>
            <span class="author-block">
              <span class="author-name">Jingcheng Hu</span><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <span class="author-name">Qiufeng Wang</span><sup>1</sup>,
            </span>
            <span class="author-block">
              <span class="author-name">Hanshan Zhang</span><sup>1</sup>,
            </span>
            <span class="author-block">
              <span class="author-name">Zili Wang</span><sup>1</sup>,
            </span>
            <span class="author-block">
              <span class="author-name">Shijie Xuyang</span><sup>1,2</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Yuantao Fan</span><sup>1</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Shuigeng Zhou</span><sup>2</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Xiangyu Zhang</span><sup>1,4</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Daxin Jiang</span><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>StepFun</span>
            <span class="author-block"><sup>2</sup>Fudan University</span>
            <span class="author-block"><sup>3</sup>Tsinghua University</span>
            <span class="author-block"><sup>4</sup>Megvii Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <figure class="image mb-5"> <!-- mb-5 控制下边距 -->
          <img src="./static/images/img1.png" alt="img1">
          <figcaption class="mt-2" style="color: #333">
            <strong style="font-weight: 900; color: #000;">The hyperparameter space for models with 400M parameters trained on 40B tokens (left) and 1B parameters 
            trained on 100B tokens (right).</strong> Optimal points represent the lowest training loss for each learning rate and 
            batch size pair, while contour lines depict the relative loss differences from these optima.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="abstract-box has-background-white-bis p-6">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 elegant-title">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization.
              Through extensive empirical studies involving grid search across diverse configurations, we discover universal scaling laws
              governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch
              size scales primarily with data sizes. 
            </p>
            <p>
              Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. 
              This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. 
              Its estimated values on the test set are merely 0.07% away from the globally optimal LLM performance found via exhaustive search.  
            </p>
            <p>
              These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. 
              To our best known, this is the first work that unifies different model shapes and structures, 
              such as Mixture-of-Experts models and dense transformers, 
              as well as to establish optimal hyperparameter scaling laws across diverse data distributions. 
              This exhaustive optimization process demands substantial computational resources, 
              utilizing nearly <b>one million</b> NVIDIA H800 GPU hours to train <b>3,700</b> LLMs of varying sizes and hyperparameters from scratch and consuming approximately <b>100 trillion tokens</b> in total. 
              To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 elegant-title">Loss Landscape Convexity</h2>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> <!-- mb-5 控制下边距 -->
          <img src="./static/images/img2.png" alt="img2">
          <figcaption class="mt-2" style="color: #333">
            <strong style="font-weight: 900; color: #000;">Learning Rate vs. Batch Size Loss Landscape Analysis for 1B Model (Trained on 100B Tokens):</strong> 
            Scatter Plots and 3D Surface Visualizations of Hyperparameter Sensitivity.
          </figcaption>
        </figure>
      </div>
    </div>

    <h2 class="title is-3 elegant-title">Generalization of the Step Law</h2>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> <!-- mb-5 控制下边距 -->
          <img src="./static/images/img3.png" alt="img3">
          <figcaption class="mt-2" style="color: #333">
            <strong style="font-weight: 900; color: #000;">Topological Invariance Across Varied Model Shape.</strong>
          </figcaption>
        </figure>
      </div>
    </div>

    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> <!-- mb-5 控制下边距 -->
          <img src="./static/images/img4.png" alt="img4">
          <figcaption class="mt-2" style="color: #333">
            <strong style="font-weight: 900; color: #000;">Validation loss landscapes of MoE models under varying sparsity ratios N<sub>a</sub>/N.</strong>
            Left: Low sparsity N<sub>a</sub>/N=0.27. Middle: Medium sparsity N<sub>a</sub>/N=0.58. 
            Right: Reduced model depth D=8.0B at medium sparsity. 
            Our method consistently approximates global minima across sparsity regimes.
          </figcaption>
        </figure>
      </div>
    </div>
    
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> <!-- mb-5 控制下边距 -->
          <img src="./static/images/img5.png" alt="img5">
          <figcaption class="mt-2" style="color: #333">
            <strong style="font-weight: 900; color: #000;">Configuration Space Analysis under Different Data Recipes.</strong>
            Our method demonstrates stable convergence patterns across varying data compositions.
          </figcaption>
        </figure>
      </div>
    </div>

    <h2 class="title is-3 elegant-title">Experimental Details</h2>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> <!-- mb-5 控制下边距 -->
          <img src="./static/images/img6.png" alt="img6">
          <figcaption class="mt-2" style="color: #333">
            <strong style="font-weight: 900; color: #000;">Comparison of learning rate schedules.</strong> These contour
            plots illustrate two distinct learning rate schedules. <strong style="font-weight: 900; color: #000;">Blue
            contours</strong> represent the <i>conventional decay schedule</i>, where the
            minimum learning rate min_lr is set to one-tenth of the maximum
            learning rate max_lr/10. <strong style="font-weight: 900; color: #000;">Red contours</strong> depict our
            proposed <i>final learning rate schedule</i>, with a constant
            minimum learning rate of min_lr = 10<sup>-5</sup>. The visualization reveals
            that the conventional decay method leads to a discernible <strong style="font-weight: 900; color: #000;">leftward
            bias</strong> in the optimal learning rate range, indicated by the shift of the lowest
            loss region towards lower learning rates in the blue contours compared to
            the red.
          </figcaption>
        </figure>
      </div>
    </div>


    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> <!-- mb-5 控制下边距 -->
          <img src="./static/images/img7.png" alt="img7">
          <figcaption class="mt-2" style="color: #333">
            <strong style="font-weight: 900; color: #000;">
            <p>(a) Scatter points indicate empirical optimal learning rate vs. batch size for model scale N;</p>
            <p>(b) Analogous results for dataset scale D.</p>
            </strong>
            Curves show our hp-scaling law predictions, with shaded regions representing parameter uncertainty bounds from the sampling-based fitting strategy.
            Both plots use double logarithmic scaling (1912 training samples).
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>



<!-- 工具 -->
<section class="section">
  
  <div class="container is-max-desktop">
    <h2 class="title is-3 elegant-title">Step Law Tools</h2>
    <div class="tool-container">
      <div class="tool-loading" id="toolLoader">
        <div class="lds-ring">
          <div></div><div></div><div></div><div></div>
        </div>
      </div>
      <iframe src="tool.html" 
              class="tool-iframe"
              loading="lazy"
              title="Hyperparameter Optimization Tool"
              onload="document.getElementById('toolLoader').style.display='none'"></iframe>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 elegant-title">BibTeX</h2>
    <pre><code>@article{,
  author    = {Houyi Li, Wenzhen Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan,
              Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang},
  title     = {Predictable Scale: Part I — Optimal Hyperparameter Scaling Law in Large Language Model Pretraining},
  journal   = {arXiv},
  year      = {2025},
}</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io">Nerfies</a> project page. 
            If you want to reuse their <a href="https://github.com/nerfies/nerfies.github.io">source code</a>, 
            please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
