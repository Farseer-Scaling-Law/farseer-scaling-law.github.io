<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Farseer: A Refined Scaling Law in Large Language Models">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Predictable Scale: Part I</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

  <style>
    .katex { font-size: 1.1em; }
    .formula-box { padding: 1rem; background: #f8f9fa; }
  </style>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero" style="margin: 0.1rem 0 !important; padding: 0.2rem 0.2rem !important;">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            <span class="highlight-title">Predictable Scale: Part I — Optimal Hyperparameter Scaling Law in 
            Large Language Model Pretraining</span>
          </h1>
            <style>
            /* 动态渐变效果 */
            .highlight-title {
              background: linear-gradient(
                45deg,
                #003f5c 0%,
                #374c80 12.5%,
                #7a5195 25%,
                #bc5090 37.5%,
                #ef5675 50%,
                #f97170 62.5%,
                #ff7c43 75%,
                #ffa600 87.5%
              );
              background-size: 400% 400%;
              -webkit-background-clip: text;
              background-clip: text;
              color: transparent;
              font-style: italic;
              animation: gradientShift 12s ease infinite;
              text-shadow: 0 2px 4px rgba(0,0,0,0.1);
            }

            @keyframes gradientShift {
              0% { background-position: 0% 50%; }
              50% { background-position: 100% 50%; }
              100% { background-position: 0% 50%; }
            }
            
            @media (prefers-reduced-motion: reduce) {
              .highlight-title {
                animation: none;
                background: #FF6B6B; /* 降级为纯色 */
              }
            }
            /* 保持原有字体样式 */
            .publication-title {
              font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            }
            </style>
          <div class="publication-authors">
            <span class="author-block">
              <span class="author-name">Houyi Li</span><sup style="font-size: 0.6rem;">1,2</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Wenzhen Zheng</span><sup style="font-size: 0.6rem;">1</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Qiufeng Wang</span><sup style="font-size: 0.6rem;">1</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Zhenyu Ding</span><sup style="font-size: 0.6rem;">3</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Haoying Wang</span><sup style="font-size: 0.6rem;">3</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Zili Wang</span><sup style="font-size: 0.6rem;">1</sup>
            </span>
            <div style="width: 100%; height: 0;"></div>
            <span class="author-block">
              <span class="author-name">Shijie Xuyang</span><sup style="font-size: 0.6rem;">1,2</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Ning Ding</span><sup style="font-size: 0.6rem;">3</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Shuigeng Zhou</span><sup style="font-size: 0.6rem;">2</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Xiangyu Zhang</span><sup style="font-size: 0.6rem;">1,4</sup>
            </span>
            <span class="author-block">
              <span class="author-name">Daxin Jiang</span><sup style="font-size: 0.6rem;">1</sup>
            </span>
          </div>
          <div class="institutional-affiliations" style="text-align: center; margin-top: 0.25rem; font-size: 0.9rem;">
              <span style="margin-right: 1rem;"><sup style="font-size: 0.6rem;">1</sup>StepFun</span>
              <span style="margin-right: 1rem;"><sup style="font-size: 0.6rem;">2</sup>Fudan University</span>
              <span style="margin-right: 1rem;"><sup style="font-size: 0.6rem;">3</sup>Xi’an Jiaotong University</span>
              <span><sup style="font-size: 0.6rem;">4</sup>Megvii Technology</span>
          </div>
          

            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.10972"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.10972"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Farseer-Scaling-Law/Farseer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- HF Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/Farseer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" width="24" height="24" />
                  </span>
                  <span>HF</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://wandb.ai/billzid/predictable-scale"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-chalkboard"></i>
                  </span>
                  <span>Wandb</span>
                </a>
              </span> -->
            </div>

          </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin: 0 !important; padding: 0 !important;">
  <div class="container is-max-desktop">
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> 
          <img src="./static/images/img1.png" alt="img1">
          <figcaption class="mt-2" style="color: #333; font-size: 0.875rem; line-height: 1.4">
            <strong style="font-weight: 900; color: #000;">Figure 1.</strong>
            (a) Average relative error (BPC) vs.\ model size $N$ for \textit{Farseer} (red) and Chinchilla (blue). Chinchilla, lacking high-order cross terms, fits only near the central $N$ and its error diverges as model size grows.
            In contrast, \textbf{\textit{Farseer}'s error is 232\% lower within the fitted range and remains stable across the full $N$ range.}
            (b) Chinchilla\text{'}s rule of thumb ($D/N\approx20$) is valid only at moderate budgets ($C\approx10^{20}\!-\!10^{21}$), but it underestimates the requirements for larger scale regimes. 
            In contrast, our analysis predicts a steadily increasing optimal $D/N$, which is consistent with the actual training configurations used in recent large language models (e.g., Llama 3.1,  Qwen3, etc.)
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section" style="margin: 0 0.5rem !important; padding: 0 0.5rem !important;">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="abstract-box has-background-white-bis p-6">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 elegant-title">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present <b>Farseer</b>, a novel and unified scaling law that offers unprecedented predictive accuracy 
              for Large Language Models (LLMs). Our findings demonstrate remarkable extrapolation capabilities, 
              <b>reducing prediction error by 433%</b> compared to Chinchilla's law and enabling highly accurate forecasts of 
              large-scale performance from small-scale experiments.
            </p>
            <p>
              This research represents a significant computational investment, 
              utilizing approximately <b>3 million NVIDIA H100 GPU hours</b> to train a comprehensive suite of 
              nearly <b>1,000 LLMs</b> from scratch. By systematically constructing the model loss surface L(N,D), 
              Farseer provides a robust and generalizable framework that bridges the critical scaling gap 
              between experimentation and production.
            </p>
            <p>
              To support reproducibility and advance the field of LLM pre-training, 
              we will progressively release our full dataset of loss measurements and checkpoints.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section" style="margin: 0 0.1rem !important; padding: 0 0.5rem !important;">
  <div class="container is-max-desktop">
    <!-- 文字描述部分 -->
    <div class="formula-caption">
      <p class="caption-text">
        <span class="highlight-term">Step Law</span> demonstrates that the optimal 
        batch size <span class="math-inline">$B(D)$</span> exhibits a primary dependence 
        on dataset size <span class="math-inline">$D$</span>, while the optimal learning rate 
        <span class="math-inline">$\eta(N, D)$</span> manifests a joint dependence on both 
        model parameters <span class="math-inline">$N$</span> and dataset size 
        <span class="math-inline">$D$</span>.
      </p>
    </div>

    <!-- 公式区块 -->
    <div class="formula-container">
      <div class="math-display">
      $$
        \begin{aligned}
        \eta(N, D) & = 1.79N^{-0.713}D^{0.307}  \\
        B(D)       & = 0.58D^{0.571}     
        \end{aligned}
      $$
      </div>
    </div>
  </div>
</section>

<style>
/* 公式容器样式 */
.formula-container {
  background: #f8f9fa; 
  border-radius: 30px;  
  padding: 0.6rem 0.6rem; 
  margin: 1.0rem auto 0rem;
  margin-bottom: 0rem;
  width: fit-content; 
  display: block;
  box-shadow: 0 2px 6px rgba(0, 0, 0, 0.08); 
  border: 1px solid #e0e0e0; /
}

/* 行内公式样式 */
.math-inline {
  font-family: "KaTeX_Math", sans-serif;
  color: #1a237e;
  font-style: italic;
  padding: 0 0.3rem;
  background: rgba(255, 255, 255, 0.9);
  border-radius: 4px;
  box-shadow: 0 1px 2px rgba(0,0,0,0.05);
}

/* 数学公式显示 */
.math-display {
  text-align: center;
  padding: 1rem;
}

.katex {
  font-size: 1.2rem !important;
  color: #000;
}

/* 文字描述样式 */
.formula-caption {
  border-left: 3px solid #4a90e2;
  padding-left: 1rem;
}

.caption-text {
  color: #333;
  line-height: 1.8;
  font-size: 0.95rem;
  font-family: 'Arial', sans-serif;
}

.highlight-term {
  color: #1565c0;
  font-weight: 700;
  letter-spacing: -0.3px;
}

/* 加载 KaTeX 字体 */
@import url('https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css');
</style>


<section class="section" style="margin: 0.5rem 0 !important; padding: 0.5rem 0 !important;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 elegant-title">Robustness and Data Generalization</h2>
    <div class="content-header mb-4">
      <div class="column is-full-width">
        <p class="subtitle is-6 has-text-grey-dark has-text-left" 
           style="line-height: 1.5; font-size: 1.05rem;">
           Farseer's prediction error consistently decreases as the amount of fitting data increases, demonstrating strong robustness. 
           It also exhibits excellent generalization across different data distributions.
        </p>
      </div>
    </div>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> 
          <img src="./static/images/img2.png" alt="img2">
          <figcaption class="mt-2" style="color: #333; font-size: 0.875rem; line-height: 1.4">
            <strong style="font-weight: 900; color: #000;">Figure 2. Robustness and data distribution generalizability of \textit{Farseer}. </strong> 
            (a) Relative error on excluded 6.4 B models as a function of the maximum model size included in the fitting data, 
            assessing robustness to fitting data volume. 
            (b) Relative error on excluded 3.2 B models trained with an English\text{-}Chinese data recipe, 
            demonstrating structural generalizability to different data mixes. 
            Circle size and adjacent numbers indicate the number of model\text{-}size points used for fitting in each case.
          </figcaption>
        </figure>
      </div>
    </div>
    <div class="content-header mb-4">
      <div class="column is-full-width">
        <p class="subtitle is-6 has-text-grey-dark has-text-left" 
           style="line-height: 1.5; font-size: 1.05rem;">
          <b>Left Figure:</b> The prediction error for Farseer decreases significantly as the amount of fitting data increases. 
          For instance, when the upper bound for model size in the fitting data is expanded from 1.9B to 5.4B parameters, 
          the relative prediction error for a 6.4B model drops from 0.6% to 0.058%, showcasing its robustness to data quantity.
          <b>Right Figure:</b> On bilingual English-Chinese (EN-ZH) data, as the amount of fitting data increases, 
          the prediction error for a 3.2B model converges to 0.076%. This indicates its generalization capability across different data distributions, 
          effectively capturing the scaling patterns of cross-lingual data.
        </p>
      </div>
    </div>


    <h2 class="title is-3 elegant-title">Extrapolation</h2>
    <div class="content-header mb-4">
      <div class="column is-full-width">
        <p class="subtitle is-6 has-text-grey-dark has-text-left" 
           style="line-height: 1.5; font-size: 1.05rem;">
           <b>Farseer's</b> average relative error is just 0.50%, 
           whereas the Chinchilla scaling law exhibits an average relative error of 2.68%, a 433% increase.
        </p>
      </div>
    </div>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> 
          <img src="./static/images/img3.png" alt="img3">
          <figcaption class="mt-2" style="color: #333; font-size: 0.875rem; line-height: 1.4">
            <strong style="font-weight: 900; color: #000;">Figure 3. Extrapolation of \textit{Farseer}.</strong>
            Blue circles represent the grid of $$(N,D$$ employed to fit. 
            Red stars denote validation points beyond that distribution, including a 25.1 B model, larger dataset sizes, 
            and off-grid combinations. Annotated percentages give the relative errors of each extrapolated point.
          </figcaption>
        </figure>
      </div>
    </div>


    <h2 class="title is-3 elegant-title">Comparison with Chinchilla's Fitting Formula</h2>
    <div class="content-header mb-4">
      <div class="column is-full-width">
        <p class="subtitle is-6 has-text-grey-dark has-text-left" 
           style="line-height: 1.5; font-size: 1.05rem;">
           Besides, our research findings reveal that this scaling law not only applies to dense models 
           but also generalizes effectively to MoE models with varying sparsity levels, 
           demonstrating robust generalization capabilities.
        </p>
      </div>
    </div>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5">
          <img src="./static/images/img4.png" alt="img4">
          <figcaption class="mt-2" style="color: #333; font-size: 0.875rem; line-height: 1.4">
            <strong style="font-weight: 900; color: #000;">Figure 4.</strong>
            <!-- Left: Low sparsity N<sub>a</sub>/N=0.27. Middle: Medium sparsity N<sub>a</sub>/N=0.58. 
            Right: Medium sparsity at D=8.0B. 
            Our method consistently approximates global minima across sparsity regimes.
             -->
             Empirical BPC values (Ground Truth) are plotted alongside fits obtained from \textit{Farseer} and Chinchilla. 
             \textit{Farseer} yields predictions that lie almost exactly on the ground truth curve, 
             whereas Chinchilla's fit exhibits systematic under‐ and over‐estimations, particularly at small and large $D$.
          </figcaption>
        </figure>
      </div>
    </div>
    

    <h2 class="title is-3 elegant-title">Comparison of Optimal Compute Allocation</h2>
    <div class="content-header mb-4">
      <div class="column is-full-width">
        <p class="subtitle is-6 has-text-grey-dark has-text-left" 
           style="line-height: 1.5; font-size: 1.05rem;">
           Under the condition C≈6ND, Chinchilla recommends a fixed D/N≈20, 
           which is only applicable for medium compute budgets ($10^20−10^21FLOPs).
           Farseer,however, predict that the optimal D/N ratio continuously increases with the compute budget, 
           which is fundamentally consistent with the training configurations of large models like Llama 3.1 and Qwen 3.
        </p>
      </div>
    </div>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> 
          <img src="./static/images/img5.png" alt="img5">
          <figcaption class="mt-2" style="color: #333; font-size: 0.875rem; line-height: 1.4">
            <strong style="font-weight: 900; color: #000;">Figure 5.</strong>
            <!-- left panel shows bilingual data, center panel combines code and mathematical data, 
            and right panel focuses on code-dominated data. 
            Each configuration trained 45 models from scratch with identical settings except batch size and learning rate, 
            totaling 135 models across three data distributions. The Global Minimum represents the ground-truth 
            lowest Final Train Smoothed Loss obtained through grid search, while Step Law-predicted optimal 
            batch sizes and learning rates consistently fall within +0.125% to 0.25% of the minimum loss across 
            all data recipes. -->
            <!-- Our method demonstrates stable convergence patterns across varying data compositions. -->
          </figcaption>
        </figure>
      </div>
    </div>

    <h2 class="title is-3 elegant-title">Point, Line, and Surface Comparison</h2>

    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> 
          <img src="./static/images/img6.png" alt="img6">
          <figcaption class="mt-2" style="color: #333; font-size: 0.875rem; line-height: 1.4">
            <strong style="font-weight: 900; color: #000;">Figure 6.</strong>
            \textit{Farseer}\text{'}s normalized 3D surface of relative BPC difference between datasets with 85\% and 50\% 
            English proportions. The translucent pink plane marks $\Delta=0$: above it, the 50\%\text{-}English configuration 
            outperforms; below it, the 85\%\text{-}English configuration outperforms. Green squares show small\text{-}scale 
            experiments at individual $(N,D)$ points, and the yellow dashed curve connects several such points—conclusions 
            from these point/line comparisons do not hold at larger scales.
          </figcaption>
        </figure>
      </div>
    </div>
    <div class="content-header mb-4">
      <div class="column is-full-width">
        <p class="subtitle is-6 has-text-grey-dark has-text-left" 
           style="line-height: 1.5; font-size: 1.05rem;">
           Regions above the plane indicate that the 50\% English mixture yields lower error, 
           while regions below favor the 85\% mixture. Green squares denote individual point comparison experiments 
           at specific \((N,D)\) coordinates, and the yellow line connects several such points for a line comparison. 
           Although these smaller scale analyses can suggest one mixture is better, 
           the surface comparison reveals how those conclusions can reverse at larger scales. 
           \textit{Farseer}\text{'}s exhibits power for low-cost, high-fidelity extrapolation across 
           any two training recipes or model designs.  
        </p>
      </div>
    </div>

    <!-- <div class="content-header mb-4">
      <div class="column is-full-width">
        <p class="subtitle is-6 has-text-grey-dark has-text-left" 
           style="line-height: 1.5; font-size: 1.05rem;">
           Through logarithmic transformation of power-law relationships into linear forms, 
           parameters are fitted via the least squares method, with robustness enhanced through bootstrap sampling. 
           We provide a precise predictive formula, establishing a theoretical foundation for 
           hyperparameter configuration in LLMs pretraining.
        </p>
      </div>
    </div>
    <div class="columns is-centered mb-6">
      <div class="column is-full-width">
        <figure class="image mb-5"> 
          <img src="./static/images/img7.png" alt="img7">
          <figcaption class="mt-2" style="color: #333; font-size: 0.875rem; line-height: 1.4">
            <strong style="font-weight: 900; color: #000;">
            <p>Figure 7. (a) Scatter points indicate empirical optimal learning rate vs. batch size for model scale N; (b) Analogous results for dataset scale D.</p>
            </strong>
            Curves show our hp-scaling law predictions, with shaded regions representing parameter uncertainty bounds from the sampling-based fitting strategy.
            Each data point in the figures represents between 45 and 120 independently trained models with distinct hyperparameters. 
            Every plotted position corresponds to the optimal hyperparameter configuration (Optimal Learning Rate and Optimal Batch Size) 
            identified through grid search under varying model sizes and data scales. 
            Both plots use double logarithmic scaling (1912 training samples).
          </figcaption>
        </figure>
      </div>
    </div> -->
  </div>
</section>



<!-- 工具 -->
<section class="section" style="margin: 0 !important; padding: 0.2rem 0.2rem !important;">
  
  <div class="container is-max-desktop">
    <h2 class="title is-3 elegant-title" id="steplawtool">Step Law Tools</h2>
    <div class="tool-container">
      <div class="tool-loading" id="toolLoader">
        <div class="lds-ring">
          <div></div><div></div><div></div><div></div>
        </div>
      </div>
      <iframe src="tool.html" 
              class="tool-iframe"
              loading="lazy"
              title="Hyperparameter Optimization Tool"
              onload="document.getElementById('toolLoader').style.display='none'"></iframe>
    </div>
  </div>
</section>


<section class="section" style="padding: 2rem 0;">
  <div class="container is-max-desktop">
    <!-- 动态标题 -->
    <div class="release-header has-text-centered mb-6">
      <h2 class="title is-3 gradient-text">
        Open Source Roadmap
        <span class="pulsating-dot"></span>
      </h2>
      <p class="subtitle is-6 has-text-grey-light">Live Progress Tracking</p>
    </div>

    <!-- 卡片式表格容器 -->
    <div class="roadmap-container">
      <!-- 时间线 -->
      <div class="timeline"></div>

      <!-- 表格主体 -->
      <div class="roadmap-table">
        <!-- 表头 -->
        <div class="table-header">
          <div class="header-item">Milestone</div>
          <div class="header-item">Release Status</div>
        </div>

        <!-- 数据行 -->
        <div class="table-row completed">
          <div class="table-cell">
            <i class="fas fa-check-circle"></i>
            Predictable Scale Part I: Optimal Hyperparameter Scaling Law
          </div>
          <div class="table-cell">
            <a href="https://arxiv.org/abs/2503.04715"
              <span class="tag is-success is-rounded">
                <i class="fas fa-file-pdf"></i> ArXiv
              </span>
            </a>
          </div>
        </div>

        <div class="table-row completed">
          <div class="table-cell">
            <i class="fas fa-toolbox"></i>
            Optimal Hyperparameter Tool
          </div>
          <div class="table-cell">
            <a href = "#steplawtool"
              <span class="tag is-info is-rounded">
                <i class="fas fa-home"></i> Tool
              </span>
            </a>
          </div>
        </div>

        <div class="table-row completed">
          <div class="table-cell">
            <i class="fas fa-chart-line"></i>
            Training Dynamic of 3700 Models
          </div>
          <div class="table-cell">
            <div class="progress-container">
              <a href="https://wandb.ai/billzid/predictable-scale"
                <span class="tag is-warning">
                  <i class="fas fa-chalkboard"></i> Wandb
                </span>
              </a>
            </div>
          </div>
        </div>

        <div class="table-row completed">
          <div class="table-cell">
            <i class="fas fa-chart-line"></i>
            Train Smoothed Loss of 3700 Models
          </div>
          <div class="table-cell">
            <div class="progress-container">
              <a href="https://github.com/step-law/steplaw"
                <span class="tag is-warning">
                  <i class="fab fa-github"></i> Github
                </span>
              </a>
            </div>
          </div>
        </div>

        <div class="table-row in-progress">
          <div class="table-cell">
            <i class="fas fa-chart-line"></i>
            Training Dynamics of 3700 Models
          </div>
          <div class="table-cell">
            <div class="progress-container">
              <a href="https://huggingface.co/StepLaw">
                <span class="tag is-warning">
                  <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face" style="height: 1em; vertical-align: middle; margin-right: 4px;">
                  Hugging Face
                </span>
              </a>
            </div>
          </div>
        </div>

        <div class="table-row upcoming">
          <div class="table-cell">
            <i class="fas fa-flag-checkered"></i>
            Predictable Scale Part II
          </div>
          <div class="table-cell">
            <div class="progress-container">
              <progress class="progress is-small" value="60" max="100"></progress>
              2025.5.1
            </div>
          </div>
        </div>

        <div class="table-row upcoming">
          <div class="table-cell">
            <i class="fas fa-flag-checkered"></i>
            Predictable Scale Part III
          </div>
          <div class="table-cell">
            <span class="release-date">
              <i class="fas fa-hourglass-half"></i>
              2025.6.1
            </span>
          </div>
        </div>

        <!-- <div class="table-row upcoming">
          <div class="table-cell">
            <i class="fas fa-flag-checkered"></i>
            Predictable Scale Part IV
          </div>
          <div class="table-cell">
            <span class="release-date">
              <i class="fas fa-hourglass-half"></i>
              2025.6.15
            </span>
          </div>
        </div>

        <div class="table-row upcoming">
          <div class="table-cell">
            <i class="fas fa-flag-checkered"></i>
            Predictable Scale Part V
          </div>
          <div class="table-cell">
            <span class="release-date">
              <i class="fas fa-hourglass-half"></i>
              2025.7.15
              <span class="countdown">(98 days)</span> 
            </span>
          </div>
        </div> -->

      </div>
    </div>
  </div>
</section>

<style>
/* 动态背景 */
.roadmap-container {
  background: linear-gradient(135deg, 
    rgba(245, 245, 245, 0.95) 0%,
    rgba(255, 255, 255, 0.9) 100%);
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
  border-radius: 1.5rem;
  padding: 2rem;
  position: relative;
  overflow: hidden;
}

/* 动态标题 */
.gradient-text {
  background: linear-gradient(45deg, #00b4d8, #90e0ef);
  -webkit-background-clip: text;
  background-clip: text;
  color: transparent;
  display: inline-block;
}

.pulsating-dot {
  display: inline-block;
  width: 12px;
  height: 12px;
  background: #00b4d8;
  border-radius: 50%;
  margin-left: 1rem;
  animation: pulse 1.5s infinite;
}

/* 表格布局 */
.roadmap-table {
  display: grid;
  gap: 1rem;
}

.table-header {
  display: grid;
  grid-template-columns: 2fr 1fr;
  padding: 1rem;
  background: rgba(255, 255, 255, 0.05);
  border-radius: 0.5rem;
}

.table-row {
  display: grid;
  grid-template-columns: 2fr 1fr;
  padding: 1rem;
  border-radius: 0.75rem;
  transition: all 0.3s ease;
  position: relative;
  background: rgba(255, 255, 255, 0.02);
}

/* 交互效果 */
.table-row:hover {
  transform: translateX(10px);
  background: linear-gradient(90deg, 
    rgba(0, 180, 216, 0.1) 0%,
    transparent 100%);
}

/* 状态指示器 */
.completed .table-cell:first-child {
  color: #2dc8e3;
}

.in-progress .table-cell:first-child {
  color: #f3d025c1;
}

.upcoming .table-cell:first-child {
  color: #9aa3ab;
}

/* 进度条样式 */
.progress-container {
  display: flex;
  align-items: center;
  gap: 1rem;
}

.progress {
  flex-grow: 1;
  height: 8px;
  border-radius: 4px;
  background: rgba(255, 255, 255, 0.1);
}

.progress::-webkit-progress-value {
  background: linear-gradient(90deg, #ffd60a, #ffc300);
}

/* 时间线动画 */
@keyframes pulse {
  0% { opacity: 0.6; transform: scale(0.9); }
  50% { opacity: 1; transform: scale(1.1); }
  100% { opacity: 0.6; transform: scale(0.9); }
}

/* 响应式设计 */
@media (max-width: 768px) {
  .roadmap-table {
    gap: 1rem;
  }

  .table-row {
    grid-template-columns: 1fr;
    padding: 1rem;
  }

  .table-cell {
    padding: 0.5rem 0;
  }
}
</style>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 elegant-title">BibTeX</h2>
    <pre><code>@misc{li2025predictablescalei,
  title    = {Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining}, 
  author   = {Houyi Li and Wenzheng Zheng and Jingcheng Hu and Qiufeng Wang and Hanshan Zhang and Zili Wang and Shijie Xuyang and Yuantao Fan and Shuigeng Zhou and Xiangyu Zhang and Daxin Jiang},
  year     = {2025},
  eprint   = {2503.04715},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG},
  url      = {https://arxiv.org/abs/2503.04715}, 
}
</code></pre>
  </div>

  <div class="institutional-logos" style="margin-top: 2rem;">
    <div style="display: flex; flex-wrap: wrap; gap: 0px; width: 100%; justify-content: center;">
        <div style="margin: 10px 20px; display: flex; justify-content: center;">
            <a href="https://platform.stepfun.com" target="_blank" style="display: flex; justify-content: center;">
                <img src="./static/images/logo1.png" alt="StepFun Logo" style="height: 90px;">
            </a>
        </div>
        <div style="margin: 10px 20px; display: flex; justify-content: center;">
            <a href="https://www.fudan.edu.cn" target="_blank" style="display: flex; justify-content: center;">
                <img src="./static/images/logo2.png" alt="Fudan Logo" style="height: 90px;">
            </a>
        </div>
        <div style="margin: 10px 20px; display: flex; justify-content: center;">
            <a href="https://www.tsinghua.edu.cn" target="_blank" style="display: flex; justify-content: center;">
                <img src="./static/images/logo3.png" alt="Tsinghua Logo" style="height: 90px;">
            </a>
        </div>
        <div style="margin: 10px 20px; display: flex; justify-content: center;">
            <a href="https://www.megvii.com" target="_blank" style="display: flex; justify-content: center;">
                <img src="./static/images/logo4.png" alt="Megvii Logo" style="height: 90px;">
            </a>
        </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link" href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io">Nerfies</a> project page. 
            If you want to reuse their <a href="https://github.com/nerfies/nerfies.github.io">source code</a>, 
            please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ],
      throwOnError: false
    });
  });
</script>

</body>
</html>
